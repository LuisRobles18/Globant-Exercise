# Migrating data from CSV to SQL database using an API REST service

## Author: Luis Alberto Robles Hernandez

## Description
This project contains the following features, which solves the problems described [here](https://docs.google.com/document/d/1U3_f50Y9Fcfp4V-695zl-d0IW7aazL9z74WvNCyULMU/edit "here"):

- Move historic data from files in CSV format to a new database (SQL) by using a Rest API service which includes:
 - Batch transations in one request
 - Reading and validating CSV files
 - Inserting records from the CSV files into a SQL database located in the cloud
 - Daily backups stored in AVRO format (for each table)
 - Ability to restore certain tables from AVRO files (for each table)
 - **NOTE:** The API REST service sends its data in an encoded format, which ensures confidentiality, and integrity (along with the backup/restore features).


## Requirements
Python 3.10+ is required. And the modules required can be installed by executing the following command:
```bash
pip install -r requirements.txt
```

A MySQL server is also required. And the data definition language file is available under the *ddl* folder, which can be imported in the server.

## How to execute this application

### Creating folders
The following folders must be created (must meet the following structure). **NOTE: The backups and data folder are located at the same directory level where the README.md file is**:

- **backups** (will contain all the backups generated by the application)
- **data**
 - **incoming** (all new incoming CSV files will be located in this folder)
 - **not_added** (records that were not added successfully will be stored in a CSV file, for each table)
   - **departments**
   - **hired_employees**
   - **jobs**
   
### Executing scripts

Under the *api_service* folder, we will run the following command in order to start the API REST service (it will be running by default on port 5000):

```bash
python3 main.py
```

The previous script will receive all API calls related to inserting data into the *jobs, departments, or hired_employeees tables*. All errors and exception will be caught and logged in a **log.txt** file.

Then we execute the following script, which will analyze (in real time), if **new files** are added in the **data/incoming folder**. 

```bash
python3 analyze_folder.py
```

The read_csv_files.py script will be automatically executed (if new files are added in the folder mentioned above) which validates, and sends the data to the API server. All missing or invalid records will be logged in a generated log.txt file. Moreover, they will be moved in the **data/not_added** folder as CSV file (for each table)

To generate **daily backups** just simply execute the following command, which will generate (for each table) AVRO files inside the **backups** folder:

```bash
python3 periodic_backups.py
```

In order to restore a specific table from the generated backups, the following command must be executed, and the only argument (avro_file_path) represents the location of the AVRO file:

```bash
python3 restore_backup.py avro_file_path
```

The previous command will replace the entire table from the SQL database with the records included in the AVRO file.
